# -*- coding: utf-8 -*-
"""mathematical_modeling

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NgLzviZKg3uirw1lX_ujTKKD6ixP8pGC
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.model_selection import TimeSeriesSplit
from scipy import stats
import seaborn as sns
from typing import List, Dict, Tuple, Callable
import warnings
warnings.filterwarnings('ignore')

class SpeechCompletionTester:
    """Framework for testing different mathematical approaches to speech completion prediction"""

    def __init__(self, random_seed=42):
        np.random.seed(random_seed)
        self.results = {}

    def generate_synthetic_speech_data(self, n_batches=100, n_topics_range=(3, 8)) -> Dict:
        """Generate synthetic speech completion data with ground truth"""

        # Generate speech patterns
        patterns = ['introduction', 'development', 'climax', 'conclusion']

        data = {
            'batch_idx': [],
            'topics': [],
            'novelty_scores': [],
            'true_completion': [],
            'speech_pattern': []
        }

        for i in range(n_batches):
            # Determine speech pattern phase
            progress = i / n_batches
            if progress < 0.2:
                pattern = 'introduction'
                base_novelty = 0.8 + np.random.normal(0, 0.1)
                true_completion = progress * 0.2  # Slow start
            elif progress < 0.7:
                pattern = 'development'
                base_novelty = 0.6 + np.random.normal(0, 0.15)
                true_completion = 0.2 + (progress - 0.2) * 0.6  # Steady progress
            elif progress < 0.9:
                pattern = 'climax'
                base_novelty = 0.4 + np.random.normal(0, 0.1)
                true_completion = 0.5 + (progress - 0.7) * 1.5  # Accelerate
            else:
                pattern = 'conclusion'
                base_novelty = 0.2 + np.random.normal(0, 0.05)
                true_completion = 0.8 + (progress - 0.9) * 2.0  # Final push

            # Generate topics for this batch
            n_topics = np.random.randint(*n_topics_range)

            # Topic persistence and recurrence
            if i == 0:
                topics = [f"topic_{j}" for j in range(n_topics)]
            else:
                # Some topics persist, some are new
                prev_topics = data['topics'][-1] if data['topics'] else []
                persistent_topics = np.random.choice(prev_topics,
                                                   size=min(len(prev_topics), n_topics//2),
                                                   replace=False).tolist()
                new_topics = [f"topic_{i}_{j}" for j in range(n_topics - len(persistent_topics))]
                topics = persistent_topics + new_topics

            data['batch_idx'].append(i)
            data['topics'].append(topics)
            data['novelty_scores'].append(max(0, min(1, base_novelty)))
            data['true_completion'].append(max(0, min(1, true_completion)))
            data['speech_pattern'].append(pattern)

        return data

    def generate_real_world_scenarios(self) -> List[Dict]:
        """Generate different real-world speech scenarios"""

        scenarios = []

        # Scenario 1: Academic Lecture (Linear progression)
        lecture_data = self.generate_synthetic_speech_data(n_batches=80)
        for i, completion in enumerate(lecture_data['true_completion']):
            lecture_data['true_completion'][i] = i / len(lecture_data['true_completion'])
        scenarios.append(('Academic Lecture', lecture_data))

        # Scenario 2: Storytelling (Non-linear with climax)
        story_data = self.generate_synthetic_speech_data(n_batches=120)
        scenarios.append(('Storytelling', story_data))

        # Scenario 3: Business Presentation (Structured phases)
        business_data = self.generate_synthetic_speech_data(n_batches=60)
        # Add structured phases
        for i, completion in enumerate(business_data['true_completion']):
            phase = i / len(business_data['true_completion'])
            if phase < 0.25:  # Introduction
                business_data['true_completion'][i] = phase * 0.1
            elif phase < 0.75:  # Main content
                business_data['true_completion'][i] = 0.1 + (phase - 0.25) * 0.8
            else:  # Conclusion
                business_data['true_completion'][i] = 0.9 + (phase - 0.75) * 0.4
        scenarios.append(('Business Presentation', business_data))

        # Scenario 4: Casual Conversation (Irregular patterns)
        casual_data = self.generate_synthetic_speech_data(n_batches=200)
        # Add noise and irregularity
        for i in range(len(casual_data['true_completion'])):
            noise = np.random.normal(0, 0.1)
            casual_data['true_completion'][i] = max(0, min(1,
                casual_data['true_completion'][i] + noise))
        scenarios.append(('Casual Conversation', casual_data))

        return scenarios

    def exponential_decay_approach(self, data: Dict, decay_rate=0.2) -> List[float]:
        """Original exponential decay approach"""
        predictions = []
        topic_history = {}

        for i, (topics, novelty) in enumerate(zip(data['topics'], data['novelty_scores'])):
            # Update topic history
            for topic in topics:
                if topic in topic_history:
                    topic_history[topic]['count'] += 1
                    topic_history[topic]['last_seen'] = i
                else:
                    topic_history[topic] = {'count': 1, 'last_seen': i}

            # Calculate novelty with exponential decay
            total_novelty = 0
            for topic in topics:
                batches_ago = i - topic_history[topic]['last_seen'] + 1
                decay_factor = np.exp(-decay_rate * batches_ago)
                topic_penalty = 1 / np.sqrt(topic_history[topic]['count'])
                total_novelty += decay_factor * topic_penalty

            avg_novelty = total_novelty / len(topics) if topics else 0

            # Simple completion calculation (position + novelty factor)
            position_ratio = i / len(data['topics'])
            completion = 0.7 * position_ratio + 0.3 * (1 - avg_novelty)
            predictions.append(max(0, min(1, completion)))

        return predictions

    def power_law_approach(self, data: Dict, alpha=1.5) -> List[float]:
        """Power law decay approach"""
        predictions = []
        topic_history = {}

        for i, (topics, novelty) in enumerate(zip(data['topics'], data['novelty_scores'])):
            # Update topic history
            for topic in topics:
                if topic in topic_history:
                    topic_history[topic]['count'] += 1
                    topic_history[topic]['last_seen'] = i
                else:
                    topic_history[topic] = {'count': 1, 'last_seen': i}

            # Calculate novelty with power law decay
            total_novelty = 0
            for topic in topics:
                batches_ago = i - topic_history[topic]['last_seen'] + 1
                decay_factor = (batches_ago + 1) ** (-alpha)
                topic_penalty = 1 / np.sqrt(topic_history[topic]['count'])
                total_novelty += decay_factor * topic_penalty

            avg_novelty = total_novelty / len(topics) if topics else 0

            position_ratio = i / len(data['topics'])
            completion = 0.7 * position_ratio + 0.3 * (1 - avg_novelty)
            predictions.append(max(0, min(1, completion)))

        return predictions

    def macd_staleness_approach(self, data: Dict, alpha_fast=0.6, alpha_slow=0.2) -> List[float]:
        """MACD-based staleness approach"""
        predictions = []
        ema_fast = 0
        ema_slow = 0

        for i, novelty in enumerate(data['novelty_scores']):
            # Calculate MACD
            if i == 0:
                ema_fast = ema_slow = novelty
            else:
                ema_fast = alpha_fast * novelty + (1 - alpha_fast) * ema_fast
                ema_slow = alpha_slow * novelty + (1 - alpha_slow) * ema_slow

            staleness = max(0, (ema_slow - ema_fast) * 10)
            position_ratio = i / len(data['novelty_scores'])

            # Combine position and staleness
            completion = 0.6 * position_ratio + 0.4 * staleness
            predictions.append(max(0, min(1, completion)))

        return predictions

    def gaussian_memory_approach(self, data: Dict, sigma=3.0) -> List[float]:
        """Gaussian memory window approach"""
        predictions = []
        topic_history = {}

        for i, (topics, novelty) in enumerate(zip(data['topics'], data['novelty_scores'])):
            # Update topic history
            for topic in topics:
                if topic in topic_history:
                    topic_history[topic]['count'] += 1
                    topic_history[topic]['last_seen'] = i
                else:
                    topic_history[topic] = {'count': 1, 'last_seen': i}

            # Calculate novelty with Gaussian memory
            total_novelty = 0
            for topic in topics:
                batches_ago = i - topic_history[topic]['last_seen']
                decay_factor = np.exp(-(batches_ago ** 2) / (2 * sigma ** 2))
                topic_penalty = 1 / np.sqrt(topic_history[topic]['count'])
                total_novelty += decay_factor * topic_penalty

            avg_novelty = total_novelty / len(topics) if topics else 0

            position_ratio = i / len(data['topics'])
            completion = 0.7 * position_ratio + 0.3 * (1 - avg_novelty)
            predictions.append(max(0, min(1, completion)))

        return predictions

    def evaluate_approach(self, true_values: List[float], predictions: List[float]) -> Dict:
        """Evaluate prediction accuracy with multiple metrics"""

        true_values = np.array(true_values)
        predictions = np.array(predictions)

        metrics = {
            'mse': mean_squared_error(true_values, predictions),
            'mae': mean_absolute_error(true_values, predictions),
            'rmse': np.sqrt(mean_squared_error(true_values, predictions)),
            'r2': r2_score(true_values, predictions),
            'correlation': np.corrcoef(true_values, predictions)[0, 1],
            'early_detection_accuracy': self._early_detection_accuracy(true_values, predictions),
            'late_stage_accuracy': self._late_stage_accuracy(true_values, predictions)
        }

        return metrics

    def _early_detection_accuracy(self, true_values: np.ndarray, predictions: np.ndarray) -> float:
        """Accuracy in early stages (first 30% of speech)"""
        cutoff = int(0.3 * len(true_values))
        early_true = true_values[:cutoff]
        early_pred = predictions[:cutoff]
        return 1 / (1 + mean_squared_error(early_true, early_pred))

    def _late_stage_accuracy(self, true_values: np.ndarray, predictions: np.ndarray) -> float:
        """Accuracy in late stages (last 30% of speech)"""
        cutoff = int(0.7 * len(true_values))
        late_true = true_values[cutoff:]
        late_pred = predictions[cutoff:]
        return 1 / (1 + mean_squared_error(late_true, late_pred))

    def cross_validate_approach(self, data: Dict, approach_func: Callable,
                              cv_splits=5, **approach_params) -> Dict:
        """Time series cross-validation for approach"""

        tscv = TimeSeriesSplit(n_splits=cv_splits)
        cv_scores = {
            'mse': [], 'mae': [], 'r2': [], 'correlation': []
        }

        # Convert data to arrays for easier splitting
        topics_array = np.array(data['topics'], dtype=object)
        novelty_array = np.array(data['novelty_scores'])
        completion_array = np.array(data['true_completion'])

        for train_idx, test_idx in tscv.split(novelty_array):
            # Create train and test data dictionaries
            train_data = {
                'topics': topics_array[train_idx].tolist(),
                'novelty_scores': novelty_array[train_idx].tolist(),
                'true_completion': completion_array[train_idx].tolist()
            }

            test_data = {
                'topics': topics_array[test_idx].tolist(),
                'novelty_scores': novelty_array[test_idx].tolist(),
                'true_completion': completion_array[test_idx].tolist()
            }

            # Train on train set, predict on test set
            predictions = approach_func(test_data, **approach_params)
            metrics = self.evaluate_approach(test_data['true_completion'], predictions)

            for metric in cv_scores:
                cv_scores[metric].append(metrics[metric])

        # Calculate mean and std for each metric
        cv_results = {}
        for metric in cv_scores:
            cv_results[f'{metric}_mean'] = np.mean(cv_scores[metric])
            cv_results[f'{metric}_std'] = np.std(cv_scores[metric])

        return cv_results

    def parameter_tuning_grid_search(self, data: Dict, approach_func: Callable,
                                   param_grid: Dict) -> Tuple[Dict, Dict]:
        """Grid search for optimal parameters"""

        best_params = None
        best_score = float('inf')
        results = []

        # Generate all parameter combinations
        import itertools

        param_names = list(param_grid.keys())
        param_values = list(param_grid.values())

        for param_combination in itertools.product(*param_values):
            params = dict(zip(param_names, param_combination))

            # Cross-validate with these parameters
            cv_results = self.cross_validate_approach(data, approach_func, **params)

            # Use MSE as primary metric for optimization
            score = cv_results['mse_mean']

            results.append({
                'params': params.copy(),
                'cv_score': score,
                'cv_results': cv_results
            })

            if score < best_score:
                best_score = score
                best_params = params.copy()

        return best_params, results

    def statistical_significance_test(self, predictions1: List[float],
                                    predictions2: List[float],
                                    true_values: List[float]) -> Dict:
        """Test statistical significance between two approaches"""

        errors1 = np.abs(np.array(predictions1) - np.array(true_values))
        errors2 = np.abs(np.array(predictions2) - np.array(true_values))

        # Wilcoxon signed-rank test
        statistic, p_value = stats.wilcoxon(errors1, errors2, alternative='two-sided')

        return {
            'statistic': statistic,
            'p_value': p_value,
            'significant': p_value < 0.05,
            'mean_error_1': np.mean(errors1),
            'mean_error_2': np.mean(errors2),
            'effect_size': (np.mean(errors1) - np.mean(errors2)) / np.sqrt((np.var(errors1) + np.var(errors2)) / 2)
        }

    def run_comprehensive_test(self, save_results=True) -> Dict:
        """Run comprehensive test across all approaches and scenarios"""

        print("🚀 Starting Comprehensive Mathematical Approach Testing...")

        # Get test scenarios
        scenarios = self.generate_real_world_scenarios()

        # Define approaches to test
        approaches = {
            'Exponential Decay': self.exponential_decay_approach,
            'Power Law': self.power_law_approach,
            'MACD Staleness': self.macd_staleness_approach,
            'Gaussian Memory': self.gaussian_memory_approach
        }

        # Parameter grids for tuning
        param_grids = {
            'Exponential Decay': {'decay_rate': [0.1, 0.2, 0.3, 0.5]},
            'Power Law': {'alpha': [1.0, 1.5, 2.0, 2.5]},
            'MACD Staleness': {
                'alpha_fast': [0.4, 0.6, 0.8],
                'alpha_slow': [0.1, 0.2, 0.3]
            },
            'Gaussian Memory': {'sigma': [2.0, 3.0, 4.0, 5.0]}
        }

        all_results = {}

        for scenario_name, scenario_data in scenarios:
            print(f"\n📊 Testing Scenario: {scenario_name}")
            scenario_results = {}

            for approach_name, approach_func in approaches.items():
                print(f"  Testing {approach_name}...")

                # Parameter tuning
                if approach_name in param_grids:
                    best_params, tuning_results = self.parameter_tuning_grid_search(
                        scenario_data, approach_func, param_grids[approach_name]
                    )
                    print(f"    Best params: {best_params}")
                else:
                    best_params = {}

                # Cross-validation with best params
                cv_results = self.cross_validate_approach(
                    scenario_data, approach_func, **best_params
                )

                # Full prediction with best params
                predictions = approach_func(scenario_data, **best_params)
                evaluation = self.evaluate_approach(
                    scenario_data['true_completion'], predictions
                )

                scenario_results[approach_name] = {
                    'best_params': best_params,
                    'cv_results': cv_results,
                    'full_evaluation': evaluation,
                    'predictions': predictions
                }

            all_results[scenario_name] = scenario_results

        # Statistical significance testing
        print("\n📈 Running Statistical Significance Tests...")
        significance_results = {}

        for scenario_name, scenario_results in all_results.items():
            scenario_data = dict(scenarios)[scenario_name]
            significance_results[scenario_name] = {}

            approach_names = list(scenario_results.keys())
            for i, approach1 in enumerate(approach_names):
                for j, approach2 in enumerate(approach_names[i+1:], i+1):
                    pred1 = scenario_results[approach1]['predictions']
                    pred2 = scenario_results[approach2]['predictions']

                    sig_test = self.statistical_significance_test(
                        pred1, pred2, scenario_data['true_completion']
                    )

                    significance_results[scenario_name][f"{approach1} vs {approach2}"] = sig_test

        final_results = {
            'approach_results': all_results,
            'significance_tests': significance_results,
            'summary': self._generate_summary(all_results)
        }

        if save_results:
            self.results = final_results

        print("\n✅ Testing Complete!")
        return final_results

    def _generate_summary(self, results: Dict) -> Dict:
        """Generate summary of results across all scenarios"""

        summary = {}

        for approach in ['Exponential Decay', 'Power Law', 'MACD Staleness', 'Gaussian Memory']:
            approach_summary = {
                'avg_mse': [],
                'avg_r2': [],
                'avg_correlation': [],
                'best_scenarios': [],
                'worst_scenarios': []
            }

            for scenario_name, scenario_results in results.items():
                if approach in scenario_results:
                    eval_results = scenario_results[approach]['full_evaluation']
                    approach_summary['avg_mse'].append(eval_results['mse'])
                    approach_summary['avg_r2'].append(eval_results['r2'])
                    approach_summary['avg_correlation'].append(eval_results['correlation'])

            if approach_summary['avg_mse']:
                summary[approach] = {
                    'mean_mse': np.mean(approach_summary['avg_mse']),
                    'mean_r2': np.mean(approach_summary['avg_r2']),
                    'mean_correlation': np.mean(approach_summary['avg_correlation']),
                    'std_mse': np.std(approach_summary['avg_mse'])
                }

        return summary

    def plot_results(self, scenario_name: str = None):
        """Plot comparison results"""
        if not self.results:
            print("No results to plot. Run comprehensive test first.")
            return

        scenarios_to_plot = [scenario_name] if scenario_name else list(self.results['approach_results'].keys())

        fig, axes = plt.subplots(2, 2, figsize=(40, 30))
        fig.suptitle('Mathematical Approaches Comparison', fontsize=16)

        # Plot 1: MSE Comparison
        ax = axes[0, 0]
        approaches = []
        mse_values = []

        for scenario in scenarios_to_plot:
            for approach, results in self.results['approach_results'][scenario].items():
                approaches.append(f"{approach}\n({scenario})")
                mse_values.append(results['full_evaluation']['mse'])

        ax.bar(range(len(approaches)), mse_values)
        ax.set_title('Mean Squared Error (Lower is Better)')
        ax.set_xticks(range(len(approaches)))
        ax.set_xticklabels(approaches, rotation=45, ha='right')
        ax.set_ylabel('MSE')

        # Plot 2: R² Comparison
        ax = axes[0, 1]
        r2_values = []

        for scenario in scenarios_to_plot:
            for approach, results in self.results['approach_results'][scenario].items():
                r2_values.append(results['full_evaluation']['r2'])

        ax.bar(range(len(approaches)), r2_values)
        ax.set_title('R² Score (Higher is Better)')
        ax.set_xticks(range(len(approaches)))
        ax.set_xticklabels(approaches, rotation=45, ha='right')
        ax.set_ylabel('R²')

        # Plot 3: Correlation Comparison
        ax = axes[1, 0]
        corr_values = []

        for scenario in scenarios_to_plot:
            for approach, results in self.results['approach_results'][scenario].items():
                corr_values.append(results['full_evaluation']['correlation'])

        ax.bar(range(len(approaches)), corr_values)
        ax.set_title('Correlation (Higher is Better)')
        ax.set_xticks(range(len(approaches)))
        ax.set_xticklabels(approaches, rotation=45, ha='right')
        ax.set_ylabel('Correlation')

        # Plot 4: Full Predictions vs True Completion (Example)
        ax = axes[1, 1]
        if scenario_name:
            scenario_data = dict(self.generate_real_world_scenarios())[scenario_name]
            true_completion = scenario_data['true_completion']
            ax.plot(true_completion, label='True Completion', color='black')
            for approach, results in self.results['approach_results'][scenario_name].items():
                ax.plot(results['predictions'], label=approach)
            ax.set_title(f'Predictions vs True Completion ({scenario_name})')
            ax.set_xlabel('Batch Index')
            ax.set_ylabel('Completion')
            ax.legend()
        else:
            ax.set_title('Full Predictions (Select a Scenario)')
            ax.text(0.5, 0.5, 'Select a scenario to see predictions vs true completion',
                    horizontalalignment='center', verticalalignment='center', transform=ax.transAxes)


        plt.tight_layout(rect=[0, 0.03, 1, 0.95])
        plt.show()

# Example usage and testing
if __name__ == "__main__":
    # Initialize tester
    tester = SpeechCompletionTester(random_seed=42)

    # Run comprehensive testing
    results = tester.run_comprehensive_test()

    # Print summary
    print("\n" + "="*60)
    print("SUMMARY OF RESULTS")
    print("="*60)

    for approach, summary in results['summary'].items():
        print(f"\n{approach}:")
        print(f"  Mean MSE: {summary['mean_mse']:.4f} (±{summary['std_mse']:.4f})")
        print(f"  Mean R²:  {summary['mean_r2']:.4f}")
        print(f"  Mean Correlation: {summary['mean_correlation']:.4f}")

    # Plot results
    tester.plot_results()

    # Show statistical significance for one scenario
    print("\n" + "="*60)
    print("STATISTICAL SIGNIFICANCE (Business Presentation)")
    print("="*60)

    for comparison, sig_result in results['significance_tests']['Business Presentation'].items():
        print(f"\n{comparison}:")
        print(f"  p-value: {sig_result['p_value']:.4f}")
        print(f"  Significant: {sig_result['significant']}")
        print(f"  Effect size: {sig_result['effect_size']:.4f}")